---
output: pdf_document
---

## Abstract




The idea of this project is to perform a predictive modeling process applied on the data set Credit. The data set is described in page 83, and available as a CSV file at: 

The idea of this project is to perform a predictive modeling process applied on the Credit data set. This dataset reports credit card balance, credit score, income, age , gender, etc. for 400 people. This data is freely available at available as a CSV file at. The methods discused are primarily based on chapter 6: Linear Model Selection and Regularization (from " _An Introduction to Statistical Learning_" by James et al), which is freely available at [INSERT URL] .



## Introduction

Given a data set containing information pertaining to credit card balances and other factors such as income, race and gender, how could one develop a model to predict one's credit card balance?

To answer this question, the following paper will discuss several different regression methods used in the predictive modeling process. The specific methods discussed are: OLS, ridge, LASSO, PCA, and PLS. Each method will be discussed and applied separately to the Credit data. More information regarding these methods is available in sections 6.2, 6.3, 6.6, and 6.7 of chapter 6: Linear Model Selection and Regularization (from " _An Introduction to Statistical Learning_" by James et al).

We will attempt to model credit card balance as a function of 10 variables: `Income`, `Limit`, `Rating`, `Cards`, `Age`, `Education`, `Gender`, `Student`, `Married`, and `Ethnicity`. Because we are modelling this relationship via linear regression, we are inherently assuming that a linear relationship exists between our dependent variables and independent variable. We will run a total of five regressions: one for each regression method.


## Data

The data set utilized in this paper, `Credit.csv`, contains 400 observations of 12 variables. Each observation corresponds to one person, and 11 of the variables are of general interest: `Income`, `Limit`, `Rating`, `Cards`, `Age`, `Education`, `Gender`, `Student`, `Married`, `Ethnicity`, and `Balance`. The  variable `X` is an index.

This dataset is free to access online at the following url http://www-bcf.usc.edu/~gareth/ISL/Credit.csv. Alternatively, you can visit the author's GitHub repository [My Github Account](http://www.github.com/jwilber) to access it as well.

For this paper, we will use the following variables: `Income`, `Limit`, `Rating`, `Cards`, `Age`, `Education`, `Gender`, `Student`, `Married`, `Ethnicity`, and `Balance`. `Income` refers to the person's income in thousands of dollars. `Limit` refers to the person's credit card limit in dollars. `Rating` refers to the person's credit rating. `Cards` refers to the number of cards the person has. `Education` refers to the number of years of education the person has completed. `Student` and `Married` are logical, answering the question of whether the person is currently a student and whether they are married. `Ethnicity` can take the values "Asian", "Caucasian", or "African American".  `Balance` refers to the person's credit card balance in dollars. `Age` and `Gender` are self-explanatory.

In our analysis, we will treat `Balance` as the dependent variable and the other 10 variables of interest as the independent variables.




# Methods

6.2 Shrinkage Methods:
  1. Ridge Regression
  2. Lasso
  3. Ridge vs Lasso
  4. Special case
  5. Selecting parameter
6,3
  1. Principal Components Regression
    1.1 PCA
    1.2 Regression on PCA
  2. Partial Least Squares
  
  
  
# 6,2
  
  we can fit a model containing all p predictors using a technique
that constrains or regularizes the coefficient estimates, or equivalently, that
shrinks the coefficient estimates towards zero. It may not be immediately
6.2 Shrinkage Methods 215
obvious why such a constraint should improve the fit, but it turns out that
shrinking the coefficient estimates can significantly reduce their variance.
The two best-known techniques for shrinking th

The first class of techniques we will discuss fall into the domain of so-called "shrinkage" methods. These are techniques that allow us to fit a model with all $p$ predictors via a regularization method that _shrinks_ the coefficient estimates towards zero.
We will discuss two regularization methods: _ridge regression_ and the _lasso_.

### Ridge Regression

Ridge regresion builds heavily off original least squares regression, which takes the following form

$ Insert Latex Equation $

In ridge regression, we add a _regularization_ parameter to this equation as follows:

$ Insert Latex Equation: Ridge Regression $
  
This is the new equation that we minimize to find our coefficients. In effect, thi quantity allows our regression model to predict observations that have more generalizability. This equation can be broken down a follow:

$ RSS + other part $

As with OLS, the above minimization equation hope to find the $\beta$ coefficients that best fit the data. The additional parameter, the $\lambda$ paramater, i known as the _shrinkage_ parameter. It becomes small when the $\beta$ coefficients are almost zero, hence the term _shrinkage_. To control the relative impact each of the two term in our new model has on our coefficient estimates, we use the parameter $\lambda$. When $\lambda = 0$, our equation simplifies to the original $RSS$. When $\lambda to inf$, the impact of the shrinkage parameter gets stronger, making our coefficient approach 0. Thus, our coefficient estimates vary as we tweak $\lambda$. Note, also, that this shrinkage parameter is not applied to our intercept term, as the goal of the minimization is to shrink the association between our predictors and the response.

So why is any of this important? Why don't we just continue employing original least squares for analysis if it works? The primary advantage of ridge regression over least squares is a result of the bias-variance trade-off, which plays into ridge regression as follows: increasing $\lamba$ decreases the variance of our model but increases the bias. Decreasing $\lamba$ increases the variance while decreasing the variance. Because original least squares uses $\lambda = 0$, it has a high variance with no bias. Therefore, at the expense of a slight increase in bias we can substantially reduce the the variance in the predictions. This shrinking of coefficient values disallows the opportunity for one predictor to have too strong an effect on our model. This helps to curb overfitting and leads to a more accurate, generalizable model.

Additionally, least squares estimates is a very poor strategy for the case when the number of features is greater than the number of observations ($p > n$), whereas ridge will still perform well by trading variance for bias. Ridge regression is also more computationally efficient than subset selection methods, as only a single model fit is necessary.


### LASSO

_Least Absolute Shrinkage and Selector Operator_, more commonly known as _LASSO_, is a regression method similar to ridge regression. The primary difference between LASSO and ridge is that LASSO will actually shrink some coefficients down to zero, while a ridge regression model fit with p predictors will maintain all p predictors. In this way, LASSO often results in a smaller model than ridge regression and can be looked at as a feature selection method. LASSO gets this property because of its choice of penalty:

INSERT LATEX EQUATION

In fact, the only difference between ridge and LASSO is the norm choice in the regularization term. While ridge shrink the coefficients towards zero, no coefficients actually achieve a quantity of zero. On the other hand, coefficients in LASSO will beceome zero, resulting in a much more concise  and interpretable model. Technically speaking, we say that LASSO yields a sparse model (a model with only a subset of variable).



Both ridge regression and LASSO rely on the parameter $\lambda$.
Clearly, then, our choice of $\lambda$ is very important for a robust analysis - so how do we choose it? For this analysis, we'll use cross-validation, a common re-sampling technique wherein which we determine a model performance based on multiple iterations of dividing our training data into folds of training and test sets. We'll pick the value of $\lambda$ that results in the lowest error.


  
  
  
### Principal Components regression (PCR)

The third method of our analysis is principal components regression, which is a popular approach to obtain a low dimensional feature set from a dataset with a high number of dimensions. Principal components regression works by first performing principal compnonents analysis (PCA) on the data, then performing a regression on the obtained principal component features.

Recall, PCA is a dimensionality reduction technique that creates a new, uncorrelated feature set where each variable is a linear combination of variables in the previous dimension. Furthermore, the principal components are ordered by the amount of variance captured, so choosing the number of features to use is as easy as setting a threshold for captured variance and picking the number of components corresponding to that much captured variance.

Principal components regression is a straightforward application from PCA: we construct a dataset with _k_ chosen principal components and use them as feature/predictors in linear regression model. We then fit said model with least squares. The idea behind this analysis is that the principal components are often sufficient enough to explain the majority of the variability in the data while also capturing the relationhip between the predictors (principal components) and the response. It should be mentioned that it is not always the case that our components hold an associataion between the predictors and response, but it is often the case. 

A benefit of principal components regression is that it often has better results than original least squares. Moreover, because we estimate a smaller number of coefficients, we are far less prone to overfitting. That said, principal components regression lacks in that its results lack interpretability; knowing how the response changes given some combination of all of our predictor is hardly intuitive. 

Prior to performing PCR, predictors should be standardized, otherwise high-variance variables will play a larger role than warranted.



### Partial Least Squares (PLS)

Partial least squares regression is a regression technique that attempts to overcome the shortcomings of PCA regression. It is very similar to PCA regression but addresses the problem inherent to PCA regression of using prinicipal components that are possibly not related to the response value. In this way, PLS regresion can be viewed as the supervised alternative to the unsupervised PCA regression.

As with PCA regression, PLS regression first reduces the dimensions of the data by creating linear combinations of the dataset's features. However, PLS regression identifies the features in a supervised manner wherein which the response value is utilized in such a way that our newly created features are related to the response; i.e. the PLS method finds directions that explain both the response and the predictors.


In PLS, we directly utilize the simple regression coefficient for each variable, as this coefficient is proportionally correlated to the response value. For this reason, PLS will place the highest weight on variables that are most strongly related to the response value. We continue selecting directions by iteratviely carrying out the following procedure: regress each variable on the former direction, then use the residuals to find the next direction.



























# Analysis



```{r, echo=FALSE}
suppressMessages(library(xtable))
load("data/Ridge-saved-objects.Rdata")
load("data/lasso-saved-objects.Rdata")
load("data/ols-saved-objects.Rdata")
load("data/pcr-saved-objects.Rdata")
load("data/pls-saved-objects.Rdata")

v <-as.vector(ols$coefficients)[-c(1)]
w <-as.vector(ridge_pred_full)[-c(1,2)]
x <-as.vector(lasso_pred_full)[-c(1,2)]
y <-as.vector(pcr_full_coefs)
z <-as.vector(pls_full_coefs)
coef_names<-c("Income", "Limit", "Rating", 
              "Cards", "Age", "Education", 
              "GenderFemale", "StudentYes", 
              "MarriedYes","EthnicityAsian",
              "EthnicityCaucasian")
names(v)<- coef_names
names(w)<- coef_names
names(x)<- coef_names
names(y)<- coef_names
names(z)<- coef_names
v_name <- "OLS"
w_name <- "Ridge"
x_name <- "LASSO"
y_name <- "PCR"
z_name <- "PLS"
df <- data.frame(v,w,x,y,z)
names(df)<-c(v_name,w_name,x_name,y_name,z_name)
coef_tbbl<-xtable(df, type="latex")
print(coef_tbbl)
```

```{r echo=FALSE}
suppressMessages(library(xtable))
load("data/Ridge-saved-objects.Rdata")
load("data/lasso-saved-objects.Rdata")
load("data/ols-saved-objects.Rdata")
load("data/pcr-saved-objects.Rdata")
load("data/pls-saved-objects.Rdata")

names(ridge_mse)<-"MSE"
names(lasso_mse)<-"MSE"
names(pcr_mse)<-"MSE"
names(pls_mse)<-"MSE"
mse_df<-data.frame(ridge_mse,lasso_mse,pcr_mse,pls_mse)
names(mse_df)<-c("Ridge", "LASSO","PCR","PLS")
mse_tbbl<-xtable(mse_df,type="latex")
print(mse_tbbl)
```
  6.6
  6.7
# Results
  List results of analyis


# Conclusion
