---
title: "Project 2 - Predictive Modeling Process Stat 159"
author: "Jared Wilber, Austin Carango, Manuel Horta"
date: "10-28-2016"
output: ioslides_presentation
---

## Project 2 Slides

author: Jared Wilber, Austin Carango, Manuel Horta
date: October 28, 2016

## Project 2


This is a slide show for project 2, Stat 159 (at UC Berkeley)

Authors

- Jared Wilber
- Austin Carango
- Manuel Horta

## Ridge Regression


- Shrinkage Technique
- Form: Append l-2 regularization parameter to the RSS of original least squares
- What is it: Ridge regression is a regularization technique that appends an l-2 norm to the RSS of OLS.
- Minimizing this quantity has the effect of trading bias for variance

## Ridge Regresson 2
- The regularization term is controlled by the lambda parameter
- You can select the best lambda parameter with cross-validation.
- When lambda grows, the severity of the penalty increases.
- When lambda is small, it approaches original least squares (they're the same if lambda=0)
- Sends parameters towards 0 (but never actually to zero)

## Lasso

- Least Absolute Shrinkage and Selection Operator
- Shrinkage Technique
- Very similar to Ridge Regression but uses the L-1 Penalty instead
- Will actually send coefficients towards zero.


## Principal Components Regression


- Principal Components Regression is a popular technique
used in regression. It is often used for high-dimensional
datasets.

- First, scale and mean-center the data.
- Next, perform PCA on the data.
- Finally, perform original least squares regression on the
new dataset.

## Pros of Principal Components Regression

- Often used with high-dimensional datasets because it allows
us to reduce the dimensions of the data.

- Removes correlation between features.



## Cons of Principal Components Regression

- The resulting components that you are regressing on are
linear combinations of the other features, so they are essentially
impossible to interpret.
- We make an inherent assumption that these components will be related in the direction of
our dependent variable, though this isn't always the case.

## Partial Least Squares Regression

- Partial Least Squares Regression is a regression technique
that is very similar to principal components analysis, but
accounts for the association with Y downfall of PCR by ensuring that
the selected components are associated with the dependent variable

- It has the same cons as PCR with regards to

## Conclusions


- There are multiple techniques for regression
- Each technique has its own strengths
- Each technique has its own weaknesses
- Different situations call for different regression techniques
